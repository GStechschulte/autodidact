{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "import math\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Proccesses "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Gaussian Distribution\n",
    "\n",
    "A univariate guassian distribution is parametrized by two parameters:\n",
    " - 1.) Mean $\\mu$ = expresses expectation of $x$\n",
    " - 2.) Standard deviation $\\sigma$ = expresses uncertainty of $x$\n",
    "\n",
    "$x$ ~ $N(\\mu, \\sigma)$\n",
    "\n",
    "A multivariate gaussian distribution is parametrized by the same parameters as above, but in vector and matrix space:\n",
    " - 1.) Mean $\\vec{\\mu}$\n",
    " - 2.) Standard deviation = covariance matrix $\\sum$\n",
    "\n",
    "$x$ ~ $N(\\vec{\\mu}, \\sum)$\n",
    "\n",
    "One of the properties of the gaussian distribution is that it is like the _black hole_ of distributions. When you:\n",
    " - Add two gaussians =  GD\n",
    " - Conditional = GD\n",
    " - Marginalize = GD\n",
    " - Product = GD\n",
    " - Subtract = GD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate gaussian distribution\n",
    "x = np.linspace(-2, 2, 50)\n",
    "y = np.linspace(-2, 2, 50)\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "# Well, technically this example is a bivariate distribution w/covariance between x1 and x2 \n",
    "covariance = np.array([[1, 0.6],\n",
    "                      [0.6, 1]])\n",
    "\n",
    "x_mu = 0\n",
    "y_mu = 0\n",
    "\n",
    "x_variance = 0.6\n",
    "y_variance = 0.6\n",
    "\n",
    "pos = np.empty(x.shape+(2, ))\n",
    "pos[:,:,0] = x\n",
    "pos[:,:,1] = y\n",
    "\n",
    "rv = multivariate_normal([x_mu, y_mu], covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data = [go.Surface(x = x, y = y, z = rv.pdf(pos))])\n",
    "fig.update_layout(title='Multivariate Gaussian Distribution', width=720, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal Distribution\n",
    "\n",
    "The marginal distribution of a multivariate gaussian is defined by the integral over the dimension we want to marginalize over. Given this multivairate joint distribution:\n",
    "\n",
    "$p(x, y) = N({\\begin{pmatrix} \\mu_x \\\\ \\mu_y \\end{pmatrix}})$ . . .\n",
    "\n",
    "The marginal distribution is acquired by simply reprameterizing the lower dimensional gaussian with the variable $x$, or $y$ that you want the marginal of which is $\\mu_x$, $\\sum_x$:\n",
    "\n",
    "$p(x) = \\int p(x, y) dy = N(\\mu_x, \\sum_x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Distribution\n",
    "\n",
    "The conditional distribution is parameterized by:\n",
    "\n",
    "$p(x|y) = N(\\mu_{x|y}, \\sum_{x|y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Processes\n",
    "\n",
    "$P(y | x, D) = \\int_w P(y | x, w) * P(w | D) dw$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "x = np.linspace(0, 10, n)\n",
    "\n",
    "train_x = torch.linspace(0, 1, 100)\n",
    "train_y = torch.sin(train_x * (2 * math.pi))\n",
    "\n",
    "# Data\n",
    "norm = multivariate_normal(mean=np.sin(x), cov=np.eye(n) * 1e-6)\n",
    "plt.figure(figsize=(16, 6))\n",
    "#plt.plot(x, norm.rvs())\n",
    "plt.plot(train_x, train_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 samples from our gaussian process - zero mean and zero correlation\n",
    "#norm = torch.distributions.MultivariateNormal(torch.zeros(100), torch.eye(100))\n",
    "norm = multivariate_normal(mean=np.zeros(100), cov=np.eye(100))\n",
    "plt.figure(figsize=(16, 6))\n",
    "[plt.plot(train_x, norm.rvs()) for _ in range(3)]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The **Prior Probability**\n",
    "\n",
    "Defining a prior over the functions consists of defining the mean and covariance matrix (computed with the kernel parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance and Kernel Matrix\n",
    "\n",
    "Assume one training point and one testing point, what does it mean they are drawn from a GD?\n",
    " - If we say they are GD, then that means if I know the label of the training point, then that informs me about the label of the test point\n",
    "\n",
    "But how do we represent this using our features? With the covariance matrix!!\n",
    " - Similar values are to have large values and non-similar values to have small values\n",
    " - I.e., If my neighbor just sold his house for a lot of money, and I know my house is similar, then my house is likely to sell for a lot too\n",
    "\n",
    "_Critical Point_ - The covariance matrix, $\\sum$, is a positive semi-definite matrix! Where have we seen these before? KERNELS!! Gaussian processes can be kernelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squared Exponential Kernel (radial basis function) - The Brad Pitt of Kernels\n",
    "def kernel(m1, m2, l=1):\n",
    "    \"\"\"\n",
    "    l = hyperparameter that leads to smoother or non-smoother functions\n",
    "        High values = smoother\n",
    "        low values  = higher variance and more uncertainty in between training points \n",
    "    \"\"\"\n",
    "    return np.exp(-1 / (2 * l**2) * (m1[:, None] - m2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prior ##\n",
    "n = 50\n",
    "x = np.linspace(0, 10, n)\n",
    "\n",
    "# Covariance matrix is now parameterized using our RBF kernel\n",
    "cov = kernel(x, x, 0.44)\n",
    "\n",
    "# Mean = 0 and cov is now parameterized with our kernel\n",
    "norm = multivariate_normal(mean=np.zeros(n), cov=cov)\n",
    "plt.figure(figsize=(16, 6))\n",
    "[plt.plot(x, norm.rvs()) for _ in range(3)]\n",
    "plt.title('Functions sampled from our GP Prior: $mu$ = 0 and RBF Kernel')\n",
    "#plt.savefig('prior_sample.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Posterior Distribution\n",
    "\n",
    "$p(f_* | X_*, X, f) = N(f_* | \\mu_*, \\sum_*)$ \n",
    "\n",
    "where:\n",
    "\n",
    "$\\mu_* =$. . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "def posterior(x_s, X_train, y_train, l=1.0, sigma_f=1.0, sigma_y=1e-8):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        x_s = new input locations (n x d)\n",
    "        X_train = training inputs (m x d)\n",
    "        y_train = training targets (m x 1)\n",
    "        l = kernel length parameter\n",
    "        sigma_f = kernel vertical variation parameter\n",
    "        sigma_y = noise parameter\n",
    "\n",
    "    returns:\n",
    "            Posterior mean vector (n x d) and covariance matrix (n x n)\n",
    "    \"\"\"\n",
    "    # Kernel of the observations X_train\n",
    "    K = kernel(X_train, X_train, l)\n",
    "    # Kernel of observations X_train vs to-predict (unseen) x_s\n",
    "    K_s = kernel(X_train, x_s, l)\n",
    "    # Posterior covariance for x_s\n",
    "    K_ss = kernel(x_s, x_s, l)\n",
    "    K_inv = inv(K)\n",
    "\n",
    "    # Posterior mu_s\n",
    "    mu_s = K_s.T.dot(K_inv).dot(y_train)\n",
    "    # Posterior covariance\n",
    "    cov_s = K_ss - K_s.T.dot(K_inv).dot(K_s)\n",
    "\n",
    "    return mu_s, cov_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing points - domain our predicted function will infer from\n",
    "n = 200 # Number of points in the posterior\n",
    "x_s = np.linspace(0, 10, n) # Testing points\n",
    "\n",
    "# Training points - values our GP will have already seen and thus should have no uncertainty (noise-free)\n",
    "f = lambda x: np.sin(x)\n",
    "X_train = np.random.uniform(0, 10, 5)\n",
    "y_train = f(X_train)\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.xlim(0, 10)\n",
    "plt.title('Training Points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior inference\n",
    "mu_s, cov_s = posterior(x_s, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which makes sense b/c mu should be a column vector and the kernel should be a matrix\n",
    "mu_s.shape, cov_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.random.multivariate_normal(mu_s.ravel(), cov_s, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember the covariance matrix - you can compute uncertainty by taking sqrt of diagional of the kernel\n",
    "uncertainty = 1.96 * np.sqrt(np.diag(cov_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(X_train, f(X_train), 'bs', ms=8, color='red')\n",
    "[plt.plot(x_s, sample) for sample in samples]\n",
    "plt.fill_between(x_s, mu_s + uncertainty, mu_s - uncertainty, color='#dddddd' )\n",
    "plt.plot(x_s, mu_s, 'r--', lw=2)\n",
    "plt.title('Posterior Distribution Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above represents the results from our posterior distribution. We sampled 10 different functions that fit our training data (red blocks). "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "55f578ea790fce859303417df5f33993e9db329ffe5a387d7c54812db965934b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
