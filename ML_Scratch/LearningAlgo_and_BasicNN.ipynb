{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm\n",
    "\n",
    " - Optimizing the parameters of a simple linear model by \"hand\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000,\n",
      "        21.8000, 48.4000, 60.4000, 68.4000])\n"
     ]
    }
   ],
   "source": [
    "# Creating input data - tensors from lists\n",
    "\n",
    "# Celcius\n",
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "\n",
    "# Unknown\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)\n",
    "\n",
    "print(t_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inspection - linear assumption\n",
    "plt.scatter(t_u, t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "Linear model: y = wx + b\n",
    " - w = weight\n",
    " - b = bias \n",
    " \n",
    "The two may be linearly related - We can derive t_c by multiplying t_u by some coefficient and adding by a constant\n",
    " - t_c = w * t_u + b\n",
    " \n",
    "### Parameter Estimation - w & b\n",
    "\n",
    " - Overview: We have a model with some unknown parameters, and we need to estimate those parameters such that the error between the predicted output and the actual output is as small as possible \n",
    " - Error metric:\n",
    "     - Squared Loss --> Mean Squared Loss (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our model \n",
    "def model(t_u, w, b):\n",
    "    return w * t_u + b\n",
    "\n",
    "# Defining our loss function\n",
    "def loss_func(t_c, t_p):\n",
    "    squared_diff = (t_p - t_c)**2\n",
    "    return squared_diff.mean()\n",
    "\n",
    "# Instantiating our model parameters - w & b\n",
    "w = torch.ones(())\n",
    "b = torch.ones(())\n",
    "\n",
    "# Calling our model \n",
    "t_p = model(t_u, w, b)\n",
    "\n",
    "# Loss function\n",
    "loss = loss_func(t_c, t_p)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But. . .our loss is not at a minimum\n",
    "\n",
    "How do we estimate the parameters w & b such that our loss reaches a global minimum?\n",
    " - **Gradient Descent**: Optimizing the loss function with respect to the parameters\n",
    "     - Partial derivatives with respect to the loss function\n",
    "     - The derivative goes back to physics and the idea is to find the rate of change\n",
    "     \n",
    "If the rate of change is **negative**, then we need to **increase** w to minimize the loss versus if the rate of\n",
    "change is **positive**, then we need **decrease** w\n",
    "\n",
    "But, by how much? This scaling factor is known as the **learning rate**:\n",
    " - Apply a change to w that is proportional to the rate of change\n",
    " - Also wise to the change the parameters slowly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4620.8970)\n",
      "tensor(-4707.5000)\n"
     ]
    }
   ],
   "source": [
    "delta = 0.1\n",
    "\n",
    "# Rate of change for parameter w\n",
    "loss_rate_change_w = \\\n",
    "    (loss_func(model(t_u, w + delta, b), t_c) - \n",
    "     loss_func(model(t_u, w - delta, b), t_c)) / (2.0 * delta)\n",
    "\n",
    "print(loss_rate_change_w)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# Updating the w parameter\n",
    "w = w - learning_rate * loss_rate_change_w\n",
    "\n",
    "# Rate of change for parameter b\n",
    "loss_rate_change_b = \\\n",
    "    (loss_func(model(t_u, w, b + delta), t_c) - \n",
    "     loss_func(model(t_u, w, b - delta), t_c)) / (2.0 * delta)\n",
    "\n",
    "print(loss_rate_change_b)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# Updating the b parameter\n",
    "b = b - learning_rate * loss_rate_change_b\n",
    "\n",
    "# This explains our parameter updating step of a learning algorithm \n",
    "# We could sit here and continually type in new parameters for w & b until our loss is minimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives\n",
    "\n",
    "Computing the derivatives with respect to each parameter\n",
    " - Using chain rule \n",
    "\n",
    "Model:\n",
    "- t_c = w * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dloss_func(t_p, t_c):\n",
    "    dsq_loss = 2 * (t_p - t_c) / t_p.size(0)\n",
    "    return dsq_loss\n",
    "\n",
    "def dmodel_w(t_u, w, b):\n",
    "    return t_u\n",
    "\n",
    "def dmodel_b(t_u, w, b):\n",
    "    return 1.0\n",
    "\n",
    "def gradient_func(t_u, t_c, t_p, w, b):\n",
    "    dloss_dp = dloss_func(t_p, t_c)\n",
    "    # Chain rule\n",
    "    dloss_dw = dloss_dp * dmodel_w(t_u, w, b)\n",
    "    dloss_db = dloss_dp * dmodel_b(t_u, w, b)\n",
    "    \n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating to fit the model - the training loop\n",
    "\n",
    "A training iteration = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        w, b = params\n",
    "        \n",
    "        ## FORWARD PASS ##\n",
    "        t_p = model(t_u, w, b)\n",
    "        loss = loss_func(t_p, t_c)\n",
    "        \n",
    "        ## BACKWARD PASS ## - grad is a single scalar quantity for each partial derivative of the loss \n",
    "        grad = gradient_func(t_u, t_c, t_p, w, b)\n",
    "        # Updating the paramaters of the model after a backward pass \n",
    "        params = params - learning_rate * grad\n",
    "        \n",
    "        print('Epoch {}, Loss {}, w {}, b{}, grad {}'.format(epoch, float(loss), params[0], params[1], grad))\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overtraining\n",
    "\n",
    "Our training process blew up. Params is receiving updates that are too large, and their values start oscillating back and forth as each update overshoots, and the next update overcorrects even more\n",
    "\n",
    "Fix?\n",
    " - Limit the magnitude of learning_rate * grad with a smaller learning rate\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(100, 1e-2, torch.tensor([1.0, 0.0]), t_u, t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller learning rate\n",
    "training_loop(100, 1e-4, torch.tensor([1.0, 0.0]), t_u, t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Inputs\n",
    "\n",
    "Notice the loss is decreasing very very slowly meaning the updates to the parameters is TOO SMALL\n",
    " - We could make the learning rate adaptive\n",
    "\n",
    "But, the gradient term in our update is problematic:\n",
    " - First epoch gradient for weight is about 50 times larger than the gradient for the bias\n",
    "     - Thus, the weight and gradient are not scaled the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "t_un = 0.1 * t_u\n",
    "print(t_u)\n",
    "print(t_un)\n",
    "\n",
    "# Notice how my gradients have changed\n",
    "training_loop(100, 1e-2, torch.tensor([1.0, 0.0]), t_un, t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = training_loop(5000, 1e-2, torch.tensor([1.0, 0.0]), t_un, t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "t_p = model(t_un, *params)\n",
    "\n",
    "plt.plot(t_u.numpy(), t_p.detach().numpy())\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch.grad\n",
    "\n",
    "PyTorch tensors can remember where they come from and, in terms of operations and parent tensors that orginated them. They can then automatically provide the chain of derivatives of such operations with respect to their inputs \n",
    " - requires_grad is telling pytorch to track the entire family tree of tensors resulting from operations on params\n",
    "     - Any tensor that will have params as an anscestor will have access to the chain of functions that were called to from params to that tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model and loss function remain the same, but our parameter initialization IS DIFFERENT\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "#params.grad is None\n",
    "\n",
    "# Forward Pass\n",
    "loss = loss_func(model(t_u, *params), t_c)\n",
    "# Backward Pass\n",
    "loss.backward()\n",
    "\n",
    "params.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulating grad functions\n",
    "\n",
    "Calling backward will lead to derivatives that **accumulate** at leaf nodes. We need to **zero** the gradient explicitly after using it for parameter updates\n",
    " - If backward was called earlier, the loss is evaluated again, backward is called again, and the gradient at each leaf is accumulated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.grad is not None:\n",
    "    params.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        # Zeroing the gradient\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "        \n",
    "        ## FORWARD PASS ##\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_func(t_p, t_c)\n",
    "        \n",
    "        ## BACKWARD PASS ## \n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating params in place - subtracting our updated params from the previous params\n",
    "        with torch.no_grad():\n",
    "            params -= learning_rate * params.grad\n",
    "        \n",
    "        \n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch {}, Loss {}'.format(epoch, float(loss)))\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(5000, \n",
    "              1e-2,\n",
    "              torch.tensor([1.0, 0.0], requires_grad=True),\n",
    "              t_un, \n",
    "              t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "\n",
    "PyTorch optimizer abstracts the optimization strategy away from user code:\n",
    " - Saves us from having to update each and every parameter to our model ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimizer\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "t_p = model(t_u, *params)\n",
    "loss = loss_func(t_p, t_c)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# The value of params is updated by calling .step()\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        ## FORWARD PASS ##\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_func(t_p, t_c)\n",
    "        \n",
    "        ## BACKWARD PASS ## \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch {}, Loss {}'.format(epoch, float(loss)))\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "training_loop(5000,\n",
    "              optimizer,\n",
    "              params, \n",
    "              t_un, \n",
    "              t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model using .nn module\n",
    "\n",
    "Revisiting the code and linear model from the Jupyter notebook \"Learning_Algorithm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c).unsqueeze(1) # <1>\n",
    "t_u = torch.tensor(t_u).unsqueeze(1) # <1>\n",
    "\n",
    "t_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_u_train = t_u[train_indices]\n",
    "t_c_train = t_c[train_indices]\n",
    "\n",
    "t_u_val = t_u[val_indices]\n",
    "t_c_val = t_c[val_indices]\n",
    "\n",
    "t_un_train = 0.1 * t_u_train\n",
    "t_un_val = 0.1 * t_u_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three arguments in the nn.Linear module - 1.) # of feature inputs 2.) # of feature outputs  3.) Inlcude bias or not (default=True)\n",
    "linear_model = nn.Linear(1, 1)\n",
    "linear_model(t_un_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linear_model.weight, '\\n')\n",
    "print(linear_model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching Inputs\n",
    "\n",
    "We have a model that takes one input and produces one output, but PyTorch nn.Module and its subclasses are designed to do so on multiple samples at the same time. To accommodate multiple samples, modules expect the zeroth dimension of the input to be the number of samples in the batch\n",
    "\n",
    "Any module in nn is written to produce outputs for a batch of multiple inputs at the same time. Thus, assuming we need to run nn.Linear on 10 samples, we can create an input tensor of size B × Nin, where B is the size of the batch and Nin is the number of input features, and run it once through the model\n",
    "\n",
    "In order to increase our batch size, we need to add an extra dimension to turn that 1D tensor into a matrix with samples in the rows and features in the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "print(t_c)\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "\n",
    "t_c = torch.tensor(t_c).unsqueeze(1) \n",
    "print(t_c)\n",
    "t_u = torch.tensor(t_u).unsqueeze(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(linear_model.parameters(),\n",
    "                     lr=1e-2)\n",
    "\n",
    "# Using parameters() method to ask any nn.Module for a list of parameters owned by it \n",
    "linear_model.parameters()\n",
    "list(linear_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train, \n",
    "                  t_c_val):\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        ## FORWARD PASS ##\n",
    "        t_p_train = model(t_u_train)\n",
    "        loss_train = loss_func(t_p_train, t_c_train)\n",
    "        t_p_val = model(t_u_val)\n",
    "        loss_val = loss_func(t_p_val, t_c_val)\n",
    "        \n",
    "        ## BACKWARD PASS ## \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 500 == 0:\n",
    "            print(\"Epoch {}\".format(epoch), \"Training Loss {}\".format(loss_train.item()),\n",
    "                  \"Validation Loss {}\".format(loss_val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(1,1)\n",
    "optimizer = optim.SGD(linear_model.parameters(), lr=1e-2)\n",
    "\n",
    "training_loop(n_epochs=3000, \n",
    "              optimizer = optimizer,\n",
    "              model = linear_model,\n",
    "              loss_fn = nn.MSELoss(),\n",
    "              t_u_train = t_u_train,\n",
    "              t_u_val = t_un_val,\n",
    "              t_c_train = t_c_train,\n",
    "              t_c_val = t_c_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Neural Network \n",
    "\n",
    "Replacing our linear model with a neural network as our approximating function\n",
    "\n",
    "Build the simplest possible neural network: a linear module, followed by an activation function, feeding into another linear module\n",
    " - Input (1d) --> Linear (13d) --> Activation Function (tanH) --> Linear (1d) --> Output\n",
    "\n",
    "The model fans out from 1 input feature to 13 hidden features, passes them through a tanh activation, and lin- early combines the resulting 13 numbers into 1 output feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = nn.Sequential(nn.Linear(1, 30),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(30, 1))\n",
    "\n",
    "seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[param.shape for param in seq_model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "        n_epochs = 3000,\n",
    "        optimizer = optimizer,\n",
    "        model = seq_model,\n",
    "        loss_fn = nn.MSELoss(),\n",
    "        t_u_train = t_un_train,\n",
    "        t_u_val = t_un_val,\n",
    "        t_c_train = t_c_train,\n",
    "        t_c_val = t_c_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_range = torch.arange(20., 90.).unsqueeze(1)\n",
    "fig = plt.figure(dpi=400)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.xlabel(\"Fahrenheit\")\n",
    "plt.ylabel(\"Celsius\")\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')\n",
    "plt.plot(t_range.numpy(), seq_model(0.1 * t_range).detach().numpy(), 'c-')\n",
    "plt.plot(t_u.numpy(), seq_model(0.1 * t_u).detach().numpy(), 'kx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
