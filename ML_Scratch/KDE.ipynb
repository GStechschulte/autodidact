{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import LeaveOneOut, train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from scipy.stats import norm\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Model - Using Kernel Density Estimation\n",
    "\n",
    "**What** is a density estimator?\n",
    "\n",
    "A density estimator is an algorithm that takes a __D__-dimensional dataset and produces an estimate of the __D__-dimensional probability distribution which that data is drawn from\n",
    " - $P(X_1, . . .X_n)$\n",
    "\n",
    "**Why** use KDE?\n",
    "\n",
    "Problem with using histogram as density estimator is that the choice of bin size and locations can lead to incaccurate estimations\n",
    "\n",
    "KDE Hyperparameters:\n",
    " - Bandwidth = \n",
    " \n",
    "KDE Runtime - KDE is known to be very computationally expensive - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover = pd.read_csv('/Users/gabestechschulte/Downloads/covertype.csv')\n",
    "cover.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "cover.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(cover.iloc[:, 0:2])\n",
    "y = np.array(cover.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.array(cover.iloc[0:1000, 0:2])\n",
    "#y = np.array(cover.iloc[0:1000, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d Feature Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(X_train[0:2000, 0]) ## Elevation\n",
    "#sns.kdeplot(X_train[:, 1]) ## Horizontal Distance to Fire Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidths = 10 ** np.linspace(0.0, 1.2, 100)\n",
    "grid = GridSearchCV(KernelDensity(kernel = 'gaussian'),\n",
    "                   {'bandwidth': bandwidths})\n",
    "\n",
    "grid.fit(X[0:2000, 0].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = KernelDensity(bandwidth = grid.best_params_['bandwidth'],\n",
    "                    kernel='gaussian').fit(X_train[0:2000, 0].reshape(-1, 1))\n",
    "logprobs = kde.score_samples(X_train[0:2000, 0].reshape(-1, 1))\n",
    "probs = np.exp(logprobs)\n",
    "len(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Model\n",
    "\n",
    "$P(Y = y | X)$ $\\propto$ $P(X | Y)*P(Y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KDEClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, bandwidth=1.0, kernel='gaussian'):\n",
    "        self.bandwidth = bandwidth\n",
    "        self.kernel = kernel\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # Unique classes in the training data\n",
    "        self.classes_ = np.sort(np.unique(y))\n",
    "        \n",
    "        # Identify observations in training set for unique classes - 5 classes means 5 training sets\n",
    "        # This is used in the next steps for the likelihood function\n",
    "        training_sets = [X[y == yi] for yi in self.classes_]\n",
    "        \n",
    "        # Fit a KDE for each unique class observation in training sets: P(X | Y) aka likelihood function\n",
    "        # When \"Y = y\", how does the distribution of \"X\" change?\n",
    "        self.models_ = [KernelDensity(bandwidth = self.bandwidth,\n",
    "                                     kernel = self.kernel).fit(Xi)\n",
    "                       for Xi in training_sets]\n",
    "        \n",
    "        # Prior belief based on the number of samples: P(Y) aka prior\n",
    "        self.logpriors_ = [np.log(Xi.shape[0] / X.shape[0])\n",
    "                          for Xi in training_sets]\n",
    "        # Probs density of observing the class\n",
    "        #print(np.exp(self.logpriors_))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_probability(self, X):\n",
    "        \n",
    "        # Score samples returns the log of the probability density function (pdf)\n",
    "        # For each model, return the score on the feature vector in a transposed array\n",
    "        logprobs = np.array([model.score_samples(X)\n",
    "                            for model in self.models_]).T\n",
    "        \n",
    "        # Exponentiate since it was returned in log form\n",
    "        results = np.exp(logprobs + self.logpriors_)\n",
    "        \n",
    "        return results / results.sum(1, keepdims = True)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # Return the class that has the highest probability given the observed feature(s)\n",
    "        # Posterior = P(Y | X)\n",
    "        return self.classes_[np.argmax(self.predict_probability(X), 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.sort(np.unique(y))\n",
    "training_sets = [X[y == yi] for yi in classes]\n",
    "models = [KernelDensity(bandwidth = 1, kernel = 'gaussian').fit(Xi) for Xi in training_sets]\n",
    "logpriors = [np.log(Xi.shape[0] / X.shape[0]) for Xi in training_sets]\n",
    "logprobs = np.array([model.score_samples(X) for model in models]).T\n",
    "results = np.exp(logprobs + logpriors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(X | Y = y)$\n",
    "\n",
    "When $Y = y$, how does the distribution of $X$ change? \n",
    " - Therefore, we are looking at the change in the features distribution conditioning on the class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_sets) ## 3 unique values, so 3 training sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models ## 3 training sets, so 3 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(Y)$ = Prior belief about our unique classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(logpriors) ## probability of observing each respective class given the inputed training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(logprobs).shape ## log of the probability density function for features using the 3 training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes[np.argmax(results / results.sum(1, keepdims=True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidths = 10 ** np.linspace(0, 1.5, 100)\n",
    "grid = GridSearchCV(KDEClassifier(), {'bandwidth': [1]})\n",
    "grid.fit(X[0:2000], y[0:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
